{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cea828",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Small Language Model (TinyLlama) Using QLoRA for SQL and Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c19888",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This project presents an end-to-end pipeline for fine-tuning a Small Language Model (SLM) using\n",
    "state-of-the-art parameter-efficient techniques. The objective is to adapt a lightweight\n",
    "instruction-tuned model to perform effectively on structured reasoning tasks such as SQL query\n",
    "generation and Python code synthesis, while operating under limited GPU resources.\n",
    "\n",
    "To achieve this, QLoRA (Quantized Low-Rank Adaptation) is employed, enabling efficient training\n",
    "in 4-bit precision without significantly sacrificing model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f270451",
   "metadata": {},
   "source": [
    "## Motivation and Business Relevance\n",
    "\n",
    "Fine-tuning large language models is often computationally expensive and impractical for many\n",
    "organizations. This project demonstrates how modern PEFT techniques allow companies to:\n",
    "\n",
    "- Adapt language models to domain-specific tasks\n",
    "- Reduce infrastructure and memory requirements\n",
    "- Deploy customized models on consumer-grade hardware\n",
    "\n",
    "The resulting approach is highly relevant for real-world applications such as:\n",
    "- Automated SQL query generation\n",
    "- Code assistance tools\n",
    "- Internal analytics and data engineering workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9802e43b-9289-4291-979f-30706946a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accba96e-0f3a-41ac-94a0-538f8e077759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "2.7.1+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)           \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())   \n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea1908-a69a-4218-b0e4-78080b327245",
   "metadata": {},
   "source": [
    "## Base Model Selection\n",
    "\n",
    "The base model used in this project is **TinyLlama-1.1B-Chat**, a compact instruction-tuned\n",
    "causal language model.\n",
    "\n",
    "This model was selected due to:\n",
    "- Its strong performance relative to its size\n",
    "- Compatibility with instruction-based fine-tuning\n",
    "- Suitability for resource-constrained environments\n",
    "\n",
    "## Quantization and Memory Optimization\n",
    "\n",
    "To enable efficient training, the model is loaded using 4-bit quantization via the\n",
    "**BitsAndBytes** library. The following configuration is applied:\n",
    "\n",
    "- 4-bit NF4 quantization\n",
    "- Double quantization for improved numerical stability\n",
    "- FP16 computation for GPU acceleration\n",
    "\n",
    "This setup significantly reduces VRAM usage, making it possible to fine-tune the model on a\n",
    "single GPU without compromising training stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abe0d3d-dad9-4b45-a49c-32c85ebee43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd934a22-c20c-450a-be7c-7ce757b4752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./tinyllama_local\"\n",
    "\n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ead13",
   "metadata": {},
   "source": [
    "\n",
    "## Offline TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54e1d0-2316-4968-a95a-0780dfa59da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "local_model_path = \"./tinyllama_local\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578763e0",
   "metadata": {},
   "source": [
    "## Parameter-Efficient Fine-Tuning (LoRA)\n",
    "\n",
    "Instead of updating all model parameters, Low-Rank Adaptation (LoRA) is applied to a subset of\n",
    "attention layers (`q_proj`, `v_proj`).\n",
    "\n",
    "Key benefits of this approach include:\n",
    "- Faster training\n",
    "- Reduced risk of overfitting\n",
    "- Minimal additional storage requirements\n",
    "\n",
    "Only a small number of trainable parameters are introduced, while the base model weights remain frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63450295-f61a-4165-ad32-ffefd723d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                              \n",
    "    lora_alpha=16,                    \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM        \n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24211a0f",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Two complementary datasets are used to improve the model’s reasoning and generation capabilities:\n",
    "\n",
    "### Spider Dataset\n",
    "- Task: Natural Language → SQL Query generation\n",
    "- Purpose: Improve structured reasoning and database query formulation\n",
    "\n",
    "### MBPP Dataset\n",
    "- Task: Python code generation\n",
    "- Purpose: Enhance algorithmic reasoning and coding ability\n",
    "\n",
    "By combining these datasets, the model is exposed to both structured query logic and general-purpose\n",
    "programming patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "942c3698-77fd-4ed6-b638-9287b962202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db_id': 'department_management', 'query': 'SELECT count(*) FROM head WHERE age  >  56', 'question': 'How many heads of the departments are older than 56 ?', 'query_toks': ['SELECT', 'count', '(', '*', ')', 'FROM', 'head', 'WHERE', 'age', '>', '56'], 'query_toks_no_value': ['select', 'count', '(', '*', ')', 'from', 'head', 'where', 'age', '>', 'value'], 'question_toks': ['How', 'many', 'heads', 'of', 'the', 'departments', 'are', 'older', 'than', '56', '?']}\n"
     ]
    }
   ],
   "source": [
    "spider = load_dataset(\"spider\")\n",
    "print(spider[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b3db40-88a5-49c8-92e6-920053710a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202bd73f23674972aa97551d8ab734ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source_file': 'Benchmark Questions Verification V2.ipynb', 'task_id': 2, 'prompt': 'Write a function to find the shared elements from the given two lists.', 'code': 'def similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) ', 'test_imports': [], 'test_list': ['assert set(similar_elements((3, 4, 5, 6),(5, 7, 4, 10))) == set((4, 5))', 'assert set(similar_elements((1, 2, 3, 4),(5, 4, 3, 7))) == set((3, 4))', 'assert set(similar_elements((11, 12, 14, 13),(17, 15, 14, 13))) == set((13, 14))']}\n"
     ]
    }
   ],
   "source": [
    "mbpp = load_dataset(\"json\", data_files=r\".\\sanitized-mbpp.json\")\n",
    "print(mbpp[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd739f0",
   "metadata": {},
   "source": [
    "## Instruction Formatting\n",
    "\n",
    "All samples are converted into a conversational instruction format:\n",
    "\n",
    "### Example:\n",
    "Human: <instruction>\n",
    "Assistant: <expected output>\n",
    "\n",
    "This structure aligns with the chat-based training paradigm of the base model and improves\n",
    "instruction-following behavior during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5c4cc3-564d-4bd7-b002-7e5f46841604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cff489b9b247b2884870fbd11a735f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69db0d534cd4d8194c161b35526db34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_spider(example):\n",
    "    prompt = f\"### Human: {example['question']}\\n### Assistant: \"\n",
    "    response = example['query']  \n",
    "    return {\"text\": prompt + response}\n",
    "\n",
    "def format_mbpp(example):\n",
    "    prompt = f\"### Human: {example['prompt']}\\n### Assistant: \"\n",
    "    response = example['code']\n",
    "    return {\"text\": prompt + response}\n",
    "\n",
    "\n",
    "spider_formatted = spider[\"train\"].map(format_spider)\n",
    "mbpp_formatted = mbpp[\"train\"].map(format_mbpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d8a7a8-f507-4a97-9d69-3920c20282ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined_dataset = concatenate_datasets([spider_formatted, mbpp_formatted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ead53d-c9ee-4f32-bc79-62431768db83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb32b8de5ff74d5d933fcab94c23262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = combined_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c1647",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "The training process is managed using Hugging Face’s `Trainer` API with the following setup:\n",
    "\n",
    "- Mixed precision training (FP16)\n",
    "- Small batch size optimized for GPU memory\n",
    "- Multiple epochs to allow task adaptation\n",
    "- Checkpoint saving with retention limits\n",
    "\n",
    "This configuration balances training efficiency with model stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "284d04e6-12f7-4b78-b3ea-d82fdfd4d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama_finetuned\",  \n",
    "    overwrite_output_dir=True,        \n",
    "    num_train_epochs=5,            \n",
    "    per_device_train_batch_size=4,      \n",
    "    save_steps=500,                 \n",
    "    save_total_limit=2,     \n",
    "    learning_rate=3e-4,         \n",
    "    weight_decay=0.01,          \n",
    "    logging_dir=\"./logs\",      \n",
    "    logging_steps=50,             \n",
    "    fp16=True,             \n",
    "    load_best_model_at_end=False      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33957b-6335-439f-9f5c-8d43ad90100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3db26e-743a-4abf-958a-0b79b48432c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bee6ae87-6ddb-46f0-9f14-c067ecff3b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tinyllama_finetuned\\\\tokenizer_config.json',\n",
       " './tinyllama_finetuned\\\\special_tokens_map.json',\n",
       " './tinyllama_finetuned\\\\chat_template.jinja',\n",
       " './tinyllama_finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./tinyllama_finetuned\")\n",
    "tokenizer.save_pretrained(\"./tinyllama_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e22cc-90ab-4364-9e33-134354c78641",
   "metadata": {},
   "source": [
    "## Model Evaluation and Inference\n",
    "\n",
    "After training, the model is evaluated through inference on unseen prompts covering:\n",
    "\n",
    "- SQL query generation\n",
    "- Python programming tasks\n",
    "- Multilingual understanding (English and Spanish)\n",
    "\n",
    "Sampling-based decoding is used to ensure diverse and coherent responses while avoiding repetition.\n",
    "\n",
    "## LoRA Merge and Deployment Readiness\n",
    "\n",
    "Once fine-tuning is completed, the LoRA adapters are merged into the base model weights.\n",
    "This step produces a standalone model suitable for deployment without requiring additional PEFT layers.\n",
    "\n",
    "The final merged model can be:\n",
    "- Loaded directly for inference\n",
    "- Integrated into downstream applications\n",
    "- Served via APIs or web interfaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc8cda-249a-49df-842f-6ce26ed83ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_path = \"./tinyllama_finetuned\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=False \n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb96a10-f7bc-4534-8774-e450a1d0dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_respuesta(prompt, max_tokens=200, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    respuesta = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return respuesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da064865-b633-415c-8bad-e346c3230a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: tell wich one is the best programming langue between SQL and Python for Data Science?\n",
      "### Assistant: SELECT ln FROM language WHERE lang  =  'SQL' INTERSECT SELECT ln FROM language WHERE lang  =  'Python'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"### Human: tell wich one is the best programming langue between SQL and Python for Data Science?\\n### Assistant:\"\n",
    "respuesta = generar_respuesta(prompt)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebeba798-167b-46c2-a30f-95e9af5493b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Eres capaz de entender espanol?\n",
      "### Assistant: Yes, I can understand Spanish.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"### Human: Eres capaz de entender espanol?\\n### Assistant:\"\n",
    "respuesta = generar_respuesta(prompt)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922d600-03be-4cbd-b1e9-7f30169d6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./tinyllama_local\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"./tinyllama_finetuned\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(\"./tinyllama_merged\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tinyllama_finetuned\", use_fast=True)\n",
    "tokenizer.save_pretrained(\"./tinyllama_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf77a6f-f6cb-4e42-9e2a-ca4b32945d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_streamlit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./tinyllama_merged\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tinyllama_merged\", use_fast=True)\n",
    "model_streamlit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffa3b676-63ba-4383-b1d1-4c2388a93179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: How do I make a query in SQL where I select everyone with age above 41 years old?\n",
      "### Assistant: SELECT * FROM employees WHERE age  >  41\n"
     ]
    }
   ],
   "source": [
    "prompt = \"### Human: How do I make a query in SQL where I select everyone with age above 41 years old?\\n### Assistant:\"\n",
    "respuesta = generar_respuesta(prompt)\n",
    "print(respuesta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tinyllama_env)",
   "language": "python",
   "name": "tinyllama_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
